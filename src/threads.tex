%------------------------------------------------------------------------------
% Copyright (c) 1991-2014, Xavier Leroy and Didier Remy.
%
% All rights reserved. Distributed under a creative commons
% attribution-non-commercial-share alike 2.0 France license.
% http://creativecommons.org/licenses/by-nc-sa/2.0/fr/
%
% Translation by Eric Cooper <ecc@cmu.edu>
%------------------------------------------------------------------------------

% \chapter{\label{sec/coprocessus}Threads}
\chapter{\label{sec/coprocessus}スレッド}
\cutname{threads.html}

% A \emph{thread}, also called a \emph{lightweight process}, is a flow
% of control that can execute in parallel with other threads in the same
% program.
\emph{スレッド} とは一つのプログラムの中で互いに並列に実行できる
制御の流れであり、\emph{軽量プロセス} とも言われます。

% This chapter describes the functions that allow a program to create
% threads (\ml+Thread+ module) and synchronize by means of locks
% (\ml+Mutex+ module), conditions (\ml+Condition+ module), and
% synchronous events (\ml+Event+ module).
この章ではスレッドを作成し (\ml+Thread+ モジュール) 同期を行うプログラムを説明します。
同期にはロック (\ml+Mutex+ モジュール)、 条件変数 (\ml+Condition+ モジュール)、そして
同期イベント (\ml+Event+ モジュール) が使われます。

% \section{Introduction}
\section{イントロダクション}

% The creation of a thread is very different from the \indexvalue{fork}
% operation that creates a copy of the current process (and therefore a
% copy of the program). After a fork, the address spaces of the parent
% and child are totally disjoint, and the two processes can communicate
% only through system calls (like reading or writing a file or a pipe).
\indexvalue{fork} は現在のプロセスのコピーを
作成します (したがってプログラムのコピーも作成します) が、スレッドの作成はこれとは大きく異なります。
フォークを行った場合、親プロセスと子プロセスのアドレス空間は完全に分離し、
二つのプロセスは (ファイルやパイプに対する入出力といった) システムコールを通じてでしか
やり取りするやり取りをすることができません。

% In contrast, all the threads within a program share the same address
% space. The only information that is not shared, and differentiates one
% thread from another, is the thread's identity and its execution stack
% (along with certain system information such as the signal mask, the
% state of locks and conditions, \etc) From this viewpoint, threads
% resemble coroutines. The threads within a given program are all
% treated in the same fashion, except for the initial thread that was
% created when the program started. When this thread terminates, so do
% all the other threads and therefore the program as a whole. (Whenever
% we speak of multiple threads, we will implicitly mean threads within a
% given program.)
これに対して、あるプログラム内の全てのスレッドは同じアドレス空間を共有します。
スレッド間で共有されず、スレッド同士で異なる唯一の情報は
スレッドの ID と実行スタック (シグナルマスクやロックと条件変数の状態などの
システム情報を含む) です。
この観点では、スレッドはコルーチンに似ています。
プログラム内のスレッドは全て同じ方法で扱われますが、
プログラムが始まって最初に作られるスレッドは別です。
このスレッドが終了すると、そのプログラムが持つ全てのスレッドが終了し
したがってプログラム全体が終了します
(これから複数のスレッドといった場合、考えているプログラム内の複数のスレッドを
意味し、別のプログラム内で動いているスレッドのことは意味しないことにします)。

% But unlike coroutines, which pass control explicitly from one to another
% and cannot execute in parallel, threads can execute in parallel and
% can be scheduled preemptively by the system. From this viewpoint,
% threads resemble processes.
制御を明示的にに受け渡し並列に実行できないコルーチンと違い、
スレッドは並列に実行でき、(他のスレッドやプロセスではなく)
システムによってプリエンプティブにスケジュールされます。
この観点ではスレッドはプロセスに似ています。

% The common address space permits threads to communicate directly among
% themselves using shared memory. The fact that threads can execute in
% parallel means that they must synchronize their access to shared data,
% so that one finishes writing before the other begins reading.
% Although not necessary in principle, in practice this requires going
% through the operating system. Synchronization is often a difficult
% part of programming with threads. It can be done with locks and
% conditions, or in a higher-level fashion with events.
アドレス空間が共有されることで、スレッド同士は共有メモリを使って
直接通信することができます。
スレッドは並列に実行されるために、全てのスレッドは共有データに対するアクセスを
同期しなければいけません。
あるスレッドが書き込み終わってから別のスレッドが読み込むようにするためです。
原理上は必要なわけではありませんが、実際にはこれを行うために
オペレーティングシステムを介する必要があります。
スレッドの同期はロックや条件変数を使うか、あるいはイベントを使って高レベルに行われます。

% The advantages of threads over processes are the lower cost of
% creation and the ability to exchange large data structures simply by
% passing pointers rather than copying.
プロセスに対するスレッドの利点は低い作成コストと
大きなデータを受け渡すときにデータ全体をコピーするのではなく
ポインタを渡すだけですむことです。

% On the other hand, using threads incurs the cost of managing the
% synchronization among them, including the case of a fatal error in one
% of the threads. In particular, a thread must be careful to release its
% locks and preserve its invariant before stopping. Processes may also
% be preferable to threads when we cannot really benefit from the
% latter's advantages.
一方、スレッドを使うとスレッド同士の同期を管理しなければならず、
スレッドが致命的なエラーを出す場合にも対処する必要があります。
例えばスレッドは停止する前にロックを開放し不変量が保存されるようにする必要があります。
上記の利点の後者による恩恵があまりないのであればスレッドよりもプロセスのほうが望ましいでしょう。

% \paragraph{Implementation in {\ocaml}}
\paragraph{{\ocaml} における実装}

% To compile an application using native threads, use the following:
ネイティブスレッドを使うアプリケーションをコンパイルするには次のようにします:
\begin{lstlisting}
ocamlc -thread unix.cma threads.cma -o prog mod1.ml mod2.ml mod3.ml

ocamlopt -thread unix.cmxa threads.cmxa -o prog mod1.ml mod2.ml mod3.ml
\end{lstlisting}
% If the \ml+ocamlbuild+ tool is used, all that is needed is to add the
% following to the \ml+_tags+ file:
\ml+ocamlbuild+ ツールを使うのであれば、
\ml+_tags+ ファイルに以下の内容を追加するだけですみます:
\begin{lstlisting}
<mod{1,2,3}.ml> : thread
<prog.{native,byte}> : use_unix, thread
\end{lstlisting}
%
% If your installation does not support native threads, you can refer to
% section~\ref{sec/thread-implementation} or the manual for instructions
% how to use simulated \quotes{\textsc{vm}-level} threads.  The text
% and examples in this chapter assume native threads and do not apply,
% in general, to \textsc{vm}-level threads.
インストールされた \ocaml がネイティブスレッドをサポートしない場合、
\ref{sec/thread-implementation} 節およびマニュアルにある
シミュレートされた  \quotes{\textsc{vm} レベル} スレッドを使う方法を参照してください。
この章のこれ以降の内容はネイティブスレッドを仮定し、\textsc{vm} レベルスレッドには一般的では
成り立ちません。

% \section{Creation and termination of threads}
\section{スレッドの作成と終了}

% The functions described in this section are defined in the
% \libmodule{Thread} module.\medskip
この章で説明される関数は \libmodule{Thread} モジュールに定義されています。

% The system call \pthreadcall{create} \ml+f v+ creates a new thread that
% executes the function application \ml+f v+ and returns a \emph{thread id} that
% the caller can use to control the newly-created thread.
システムコール \pthreadcall{create} \ml+f v+ は関数適用 \ml+f v+ を実行する新しいスレッドを
作成し、スレッド ID を返します。呼び出し側はこの ID を使って新しく作られたスレッドを
制御することができます。
%
\begin{codefile}{tmpthread.mli}
type t
\end{codefile}
%
\begin{listingcodefile}{tmpthread.mli}
val $\libvalue{Thread}{create}$ : ('a -> 'b) -> 'a -> t
\end{listingcodefile}
%
% The function application executes concurrently with the other threads
% in the program. The thread terminates when the application returns and
% its result is simply ignored.  If the thread terminates with an
% uncaught exception, the exception is not propagated to any other
% thread: a message is printed on the standard error output and the
% exception is otherwise ignored.  (The other threads have proceeded
% independently and would not be able to receive the exception.)
新しいスレッドで実行される \ml+f v+ という関数適用はプログラム内の他のスレッドと並行に実行されます。
関数適用が値を返すとスレッドは終了し、返り値は無視されます。
スレッドが捕捉されないエラーによって終了したとしても、
そのエラーが他のスレッドに及ぶことはありません。
そのエラーに関するメッセージが標準エラー出力に出力されるか、エラーは完全に無視されます。
他のスレッドは独立して実行されるので、このエラーを受け取ることはできません。

% A thread can also terminate prematurely with the system call
% \pthreadcall{exit} of the \ml+Thread+ module, not to be confused with
% \ml+Pervasives.+\indexlibvalue{Pervasives}{exit} that terminates the
% entire program, {\ie} all its threads.
スレッドはこの他にも \ml+Thread+ モジュールのシステムコール \pthreadcall{exit} を呼ぶことによってでも
終了できます。 \ml+Pervasives.+\indexlibvalue{Pervasives}{exit} と混同しないようにしてください。
これはプログラム全体、つまり全てのスレッドを終了します。
%
\begin{listingcodefile}{tmpthread.mli}
val $\indexlibvalue{Thread}{exit}$ : unit -> unit
\end{listingcodefile}
%
% The initial thread of a program implicitly calls the
% \ml+Pervasives.exit+ function when it terminates.
プログラムの最初のスレッドは終了するときに暗黙のうちに \ml+Pervasives.exit+ を呼びます。

% When another thread terminates before the initial thread, it is
% deallocated immediately by the {\ocaml} runtime library.  It does not
% become a zombie as in the case of a Unix process created by \ml+fork+.
最初のスレッドが終了する前にそれ以外のスレッドが終了すると、
そのスレッドは \ocaml のランタイムライブラリによってすぐに開放されます。
\ml+fork+ で作成された Unix プロセスのようにゾンビになることはありません。

% The system call \pthreadcall{self} returns the thread id of the
% calling thread.
システムコール \pthreadcall{self} は呼び出したスレッドのスレッド ID を返します。
\begin{listingcodefile}{tmpthread.mli}
val $\libvalue{Thread}{self}$ : unit -> t
\end{listingcodefile}

% We already know enough to propose an alternative to the preceding
% model for the concurrent server that used \quotes{fork} (or
% \quotes{double fork})~---~by using a thread rather than a child process.
% To establish such a server, we introduce a variant
% \ml+Misc.co_treatment+ of the function \ml+Misc.fork_treatment+
% defined in section~\ref{sec/service}.
前の章で作成した \quotes{フォーク} と \quotes{ダブルフォーク} を使った並行サーバを、
子プロセスではなくスレッドを使うように書き換えるための準備が整いました。
そのようなサーバを作るためには、\ref{sec/service} 節で定義した \ml+Misc.fork_treatment+ に
似た \ml+Misc.co_treatment+ を作成します。

%
\begin{codefile}{misc.mli}
val co_treatment :
file_descr -> (file_descr * sockaddr -> unit) ->
file_descr * sockaddr -> unit
(** same as [sequential_treatment] but the treatment is concurrently
 performed by another thread. It is the responsibility of the treatment
 to close file_descr when completed. *)
\end{codefile}
%
\begin{listingcodefile}{misc.ml}
let co_treatment server_sock service (client_descr, _ as client) =
  try ignore (Thread.create service client)
  with exn -> close client_descr; raise exn;;
\end{listingcodefile}
%
% If the thread was successfully created, the treatment is handled
% entirely by the \ml+service+ function, including closing
% \ml+client_descr+.  Otherwise, we close the \ml+client_descr+
% descriptor, the client is abandoned, and we let the main program
% handle the error.
スレッドの作成に作成して場合、
\ml+client_descr+ を閉じることを含んだサービスは \ml+service+ 関数によって処理されます。
スレッドの作成に失敗した場合、
ディスクリプタ \ml+client_descr+ を閉じてクライアントを放棄し、
エラーに対する処理をメインプログラムに任せます。

% Note that all the difficulty of the co-server is hidden in the
% \ml+service+ function, which must handle the connection robustly until
% disconnection.  In the case of a concurrent server where the service
% is executed by another process, premature termination of the service
% due to a fatal error produces by default the desired behavior~---~closing
% the connection~---~because the system closes the file descriptors when a
% process exits.  But in the case where the service is executed by a
% thread, the descriptors of the different threads are shared by default
% and not closed at the termination of the thread.  It is therefore up
% to the thread to close its descriptors before exiting.  In addition, a
% thread cannot call \ml+Pervasives.exit+ in the case of a fatal error
% during the handling of a service, because it would stop not only the
% service but also the entire server.  Calling \ml+Thread.exit+ is often
% not a solution either, because we risk not having properly
% deallocated the thread's open resources, and in particular the
% connection.
スレッドを使ったサーバでは難しい部分が
\ml+service+ 関数に隠れていることに注意してください。
この関数は接続を切断するまでロバストに処理しなければいけません。
プロセスを使った並行サーバの例ではサービスが他のプロセスによって実行されており、
エラーが起こってサービスが途中で終了したとしても望みどおりの処理~---~接続の処理~---~が行われます。
これはプロセスが終了するとシステムがファイルディスクリプタを閉じるためです。
しかしサービスがスレッドによって実行されている場合、
デフォルトの設定ではディスクリプタはスレッド間で共有され、
スレッドの終了時に閉じられることがありません。
そのためスレッドは終了する前にディスクリプタを閉じる必要があります。
加えて、サービスは致命的なエラーのときに \ml+Pervasives.exit+ を呼ぶこともできません。
なぜならこの呼び出しはサービスだけではなくサーバ全体を終了させるからです。
致命的なエラーのときに \ml+Thread.exit+ を使っても解決にならないことが多いです。
なぜならスレッドの持つ開いたままのリソース、つまり接続が閉じられていない可能性があるからです。

% One solution consists of raising an exception to signify a fatal stop
% (for example an \ml+Exit+ exception), causing finalization code to be
% executed as it is handled.  For similar reasons, it is essential to
% block the \ml+sigpipe+ signal during the handling of a service by a
% thread, replacing the immediate termination of the thread by the
% raising of an \ml+EPIPE+ exception.
一つの解決法は終了処理を実行するコードがハンドラとして登録されている例外 (例えば \ml+Exit+ 例外)
を使うことです。
また同様の理由でサービスの実行中に \ml+sigpipe+ シグナルをブロックし、
シグナルに対する動作を動作をスレッドの終了から \ml+EPIPE+ 例外に変えておくことが重要です。


% \section{Waiting}
\section{ウェイト}

% The functions described in this section are defined in the
% \libmodule{Thread} module.\medskip
この節で説明される関数は \libmodule{Thread} モジュールに定義されています。

% The system call \pthreadcall{join} allows one thread to wait for another
% to finish.
システムコール \pthreadcall{join} を使うとあるスレッドから他のスレッドの終了を待つことができます。
%
\begin{listingcodefile}{tmpthread.mli}
val $\libvalue{Thread}{join}$ : t -> unit
\end{listingcodefile}
%
% The calling thread is suspended until the thread with the given
% thread id has terminated its execution. This function can also be used
% by the principal thread to wait for all the other threads to finish
% before terminating itself and the program. (The default behavior is to
% kill the other threads without waiting for them to terminate.)
この関数を呼んだスレッドの実行は指定された ID のスレッドの実行が終了するまで中断されます。
主スレッドがこの関数を使えば終了する前に他の全てのスレッドが終了するのを待つことができます
(この関数を使わないと主スレッドが終了した時点で他の全てのスレッドも終了します)。

% Although this call is blocking and therefore \quotes{long}, it
% is restarted automatically when a signal is received: it is
% effectively interrupted by the signal, the handler is invoked, then the
% call is restarted.  The call therefore does not return until the
% thread has really terminated, and the call never raises the \ml+EINTR+
% exception.  From the viewpoint of the {\ocaml} programmer, it behaves as
% if the signal was received at the moment when the call returns.
\ml+join+ はブロックするので \quotes{遅い} システムコールですが、
この関数はシグナルを受け取ったときに自動的に再開します。
正確にはシグナルによって実行が中断され、ハンドラが呼び出されてから、もう一度関数が呼ばれます。
そのためこの関数は指定したスレッドが本当に終了するまで返らず、
\ml+EINTR+ 例外を出すことはありません。
\ocaml プログラマの観点からは、
\ml+join+ はシグナルは関数から返った瞬間に実行されるように振る舞います。

% A thread does not return, since it is executed asynchronously.  But
% its action can be observed~---~luckily!~---~by its side effects.  For
% example, one thread can place the result of a computation in a
% reference that another thread will consult after making sure that the
% calculation has finished.  We illustrate this in the following
% example.
スレッドは非同期に実行されるので、値を返すことがありません。
しかしその動作は~---~幸運にも!~---~副作用によって観測することができます。
例えばスレッドは計算の結果を参照値に保存し、
他のスレッドは計算が終わったことを確認してからその値を読み込むことができます。
次の例がこのことを説明します。
%
\begin{codefile}{tmpthread.ml}
open Thread;;
\end{codefile}
\begin{listingcodefile}{tmpthread.ml}
exception Exited
type 'a result = Value of 'a | Exception of exn
let eval f x = try Value (f x) with z -> Exception z
let coexec (f : 'a -> 'b) (x : 'a) : unit -> 'b =
  let result = ref (Exception Exited) in
  let p = Thread.create (fun x -> result := eval f x) x in
  function () -> match (join p; !result) with
    | Value v -> v
    | Exception exn -> raise exn;;

let v1 = coexec succ 4 and v2 = coexec succ 5 in v1 () + v2 ();;
\end{listingcodefile}

% The system can suspend one thread in order to give control temporarily
% to another, or because it is waiting for a resource being used by
% another thread (locks and conditions, for example) or by another
% process (file descriptors, for example).  A thread can also suspend
% itself voluntarily.  The \ml+yield+ function allows a thread to give
% up control explicitly, without waiting for preemption by the system.
システムは他のスレッドに制御を移すため、あるいはスレッドが他のスレッドやプロセスが利用中のリソース
(例えばロックや条件変数、ファイルディスクリプタ) を待つために、スレッドを停止することができます。
スレッドは自分自身の実行を停止することができます。
\ml+yeild+ 関数を使うとシステムによるプリエンプションを待つことなく明示的に実行を停止することができます。
%
\begin{listingcodefile}{tmpthread.mli}
val $\indexlibvalue{Thread}{yield}$ : unit -> unit
\end{listingcodefile}
%
% It is a hint for the thread scheduler, but it may have no effect, for
% example if no other thread can execute immediately, the system may
% give control back to the same thread.
この関数はスレッドスケジューラに対してヒントを出しますが、なんの効果も持たない場合もあります。
例えば \ml+yield+ が呼ばれた時点で他に実行できるスレッドがない場合は、
システムは同じスレッドの実行を続けます。

% Conversely, it is not necessary to execute \ml+yield+ to permit other
% threads to execute, because the system reserves the right to execute
% the \ml+yield+ command itself at any moment.  In fact, it exercises
% this right sufficiently often to permit other threads to execute and
% to give the illusion that the threads are running in parallel, even on
% a uniprocessor machine.
逆に、他のスレッドが実行されるためには \ml+yeild+ を呼ばなくてはいけないわけではありません。
システムは \ml+yeld+ コマンドを任意のタイミングで実行する権限を持っているからです。
実際にはシステムはこの権限を頻繁に使って他のスレッドを実行し、
プロセスが一つしか無いマシンでもスレッドが並列に実行されているように見せています。

\begin{example}
% We can revisit the example of section~\ref{ex/forksearch} and modify it to use
% threads rather than processes.
\ref{ex/forksearch} 節の例をプロセスではなくスレッドを使って書き直します。
%simulated
\begin{codefile}{thpsearch.ml}
open Forksearch
\end{codefile}
%
\begin{listingcodefile}[style=numbers]{thpsearch.ml}
let rec psearch k cond v =
  let n = Array.length v in
  let slice i = Array.sub v (i * k) (min k (n - i * k)) in
  let slices = Array.init (n/k) slice in
  let found = ref false in
  let pcond v = if !found then (Thread.exit (); cond v) else cond v in
  let search v = if simple_search pcond v then found := true in $\label{prog:search}$
  let proc_list = Array.map (Thread.create search) slices in
  Array.iter Thread.join proc_list;
  !found;;
\end{listingcodefile}
%
\begin{codefile}{thpsearch.ml}
if Array.length Sys.argv > 2 then
let arr = Array.sub Sys.argv 2 (Array.length Sys.argv - 2) in
Pervasives.exit (if psearch 1 ((=) Sys.argv.(1)) arr then  0 else 1);;
\end{codefile}
%
% The function \ml+psearch k f v+ searches with \ml+k+ threads in
% parallel for an array element satisfying the function \ml+f+.
% The function \ml+pcond+ allows the search to be interrupted when an
% answer has been found.  All the threads share the same reference
% \ml+found+: they can therefore access it concurrently.  No critical
% section is required, because if different threads write to this resource
% in parallel, they write the same value.  It is important that the
% threads do not write the result of the search when it is false!
% For example, replacing line~\ref{prog:search} by
\ml+psearch k f v+ は \ml+k+ このスレッドを使って配列の要素のうち \ml+f+ を
満たすものを並列に探索します。
\ml+pcond+ は探索がすでに成功している場合にスレッドを終了させることで探索を打ち切ります。
全てのスレッドは参照 \ml+found+ を共有し、アクセスは並列に起こります。
この参照への書き込みは必ず値を \ml+true+ にするので、クリティカルセクションは存在しません。
探索対象が見つからなかったときに全体の探索結果をセットしてはいけません。
例えば~\ref{prog:search} 行目を
%
\begin{lstlisting}
let search v = found := !found && simple_search pcond v
\end{lstlisting}
%
% or even:
あるいは:
%
\begin{lstlisting}
let search v = let r = simple_search pcond v in found := !found && r
\end{lstlisting}
%
% would be incorrect.
に入れ替えると正しく動かなくなります。
% \enlargethispage{1\onelineskip} %% To avoid a widow
\end{example}
% The parallel search is interesting even on a uniprocessor machine if
% the comparison of elements could be blocked temporarily (for example by
% disk accesses or network connections).  In this case, the thread
% performing the search passes control to another and the machine can
% therefore continue the computation on another part of the array and
% return to the blocked thread when its resource is free.
要素の比較が (デイスクアクセスやネットワークへの接続で) 一時的なブロックを引き起こすならば、
並列探索はプロセスが一つしか無いマシンでも意味があります。
この場合リソースを待っているスレッドが他のスレッドに実行を譲ることで配列の他の部分の計算を続けることができ、
ブロックされたスレッドの実行はリソースが利用可能になってから再開されます。

% Access to certain elements can have significant latency, on the order
% of a second if information must be retrieved over the network.  In
% this case, the difference in behavior between a sequential search and
% a parallel search becomes obvious.
特定の要素へのアクセスがとても大きなレイテンシを持つという場合があり、
とくにネットワーク越しにデータを取得しなければいけない場合にはレイテンシが数百倍となります。
このようなケースではシーケンシャルな探索と並列な探索の差が明確になります。
\medskip
\begin{exercise}
\label{ex/qsort}
% Parallelize quicksort on arrays.
配列に対するクイックソートを並列化してください。
\end{exercise}
\begin{answer}
% Quicksort lends itself well to parallelization, because the sorting is
% done recursively on independent sub-arrays.  This can be delegated to
% threads whose only synchronization is to wait for all the threads to
% finish their sorting, in order for the sub-array to be sorted.
クイックソートは目的の配列を分割した配列を再帰的にソートしますが、
この分割した配列に対するソートは独立に行えるので並列化はしやすいです。
クイックソートを実行するスレッドにおける唯一の同期は
分割した配列のソートが終わるのを待つ部分だけです。

\begin{codefile}{thsort.ml}
let swap arr i j =
  let tmp = arr.(i) in
  arr.(i) <- arr.(j);
  arr.(j) <- tmp;;
\end{codefile}
\begin{listingcodefile}[style=numbers]{thsort.ml}
let qsort cmp arr =
  let rec qsort lo hi =
  if hi - lo > 0 then
    begin
      let mid = (lo + hi) lsr 1 in
      if cmp arr.(mid) arr.(lo) then swap arr mid lo;
      if cmp arr.(hi) arr.(mid) then
        begin
          swap arr mid hi;
          if cmp arr.(mid) arr.(lo) then swap arr mid lo
        end;
      let pivot = arr.(mid) in
      let i = ref (lo + 1) and j = ref (hi - 1) in
      while !i < !j do
        while not (cmp pivot arr.(!i)) do incr i done;
        while not (cmp arr.(!j) pivot) do decr j done;
        if !i < !j then swap arr !i !j;
      done;
      let u = Thread.create (qsort lo) (!i-1) in
      let v = Thread.create (qsort (!i+1)) hi in $\label{prog:tcreate}$
      Thread.join u; $\label{prog:tjoin}$
      Thread.join v
    end in
  qsort 0 (Array.length arr - 1);;
\end{listingcodefile}
%
\begin{codefile}{thsort.ml}
let arr = Array.sub Sys.argv 1 (Array.length Sys.argv - 1) in
qsort (<=) arr;
Array.iter print_endline arr;;
\end{codefile}
%
% It would be correct, but uninteresting, to exchange
% lines~\ref{prog:tcreate} and~\ref{prog:tjoin}.
% In effect, that would wait for the lower portion of the array to be
% sorted before starting the sort of the upper portion.  We would thus
% obtain the behavior of a sequential program, with the additional cost
% of threads but without obtaining any benefit.
\ref{prog:tcreate} 行目と~\ref{prog:tjoin} 行目を入れ替えても正しい答えは得られますが、
あまり意味がないでしょう。
配列の前半分がソートされてから後半分のソートが始まるので、
動作はシーケンシャルなプログラムと同じになりますが、スレッドの生成の分だけ実行が遅くなります。

% In practice, we should limit parallelization to a reasonable factor
% and continue sequentially after that.
並列クイックソートを実際のプログラムで使う場合には、
配列の長さがある程度よりも小さくなったらスレッドを作らずにシーケンシャルに
ソートしたほうが良いでしょう。
\end{answer}

% The other forms of suspension are tied to operating system resources.
% A thread can be suspended for a certain time by calling \ml+delay s+.
% Once \ml+s+ seconds elapse, it can be restarted.
これから紹介するスレッドを停止させる別の方法は、
オペレーティングシステムのリソースに関係しています。
スレッドは \ml+delay s+ を呼ぶことで任意の時間だけ実行を停止することができます。
実行は \ml+s+ 秒後に再開されます。
%
\begin{listingcodefile}{tmpthread.mli}
val $\indexlibvalue{Thread}{delay}$ : float -> unit
\end{listingcodefile}
%
% This primitive is provided for portability with \textsc{vm}-level threads, but
% \ml+delay s+ is simply an abbreviation for
% \ml+ignore (Unix.select [] [] [] s)+. This call, unlike \ml+join+, is
% not restarted when it is interrupted by a signal.
この関数は \textsc{vm}-レベルのスレッドでもポータブルに使うことができますが、
\ml+delay s+ は \ml+ignore (Unix.select [] [] [] s)+ の省略形に過ぎません。
\ml+join+ と違ってこの関数はシグナルで中断されても自動的に再開しません。

%% Un processus peut attendre sur un signal.
%% \begin{lstlisting}
%% val $\indexlibvalue{Thread}{wait_signal} : int -> int * process_status
%% \end{lstlisting}

% To synchronize a thread with an external operation, we can use the
% \indexvalue{select} command. Note that this will block only the
% calling thread and not the entire program.  (The \ml+Thread+ module
% redefines this function, because in the case of simulated threads
% calling the one in the \ml+Unix+ module would block the whole program
% and therefore all the threads.  It is therefore necessary to use
% \libvalue{Thread}{select} from the \ml+Thread+ module and not
% \ml+Unix.select+, even if the two are equivalent in the case of native
% threads.)
スレッドを外部の操作と同期させるために \indexvalue{select} を使うことができます。
この関数がブロックするのはプログラム全体ではなくこの関数を読んだスレッドだけであることに
注意してください。
シミュレートされたスレッドが \ml+Unix+ モジュールの \ml+select+ を呼ぶとプログラム全体すなわち
全てのスレッドをブロックしてしまうために、\ml+Thread+ モジュールはこの関数を再定義しています。
ネイティブスレッドを利用するプログラムでは \ml+Unix.select+ と \ml+Thread.select+ の動作に違いはありませんが、
この理由から \ml+Unix+ モジュールではなく \ml+Thread+ モジュールの \libvalue{Thread}{select} 関数を使うことが
必要になります。

\begin{example}
\label{ex/crible-copro}
% To make the Sieve of Eratosthenes example of section~\ref{ex/crible}
% work with threads instead of by duplication of Unix processes,
% it suffices to replace the
% lines~{\ref{prog:sievefilterfork}--\ref{prog:sievefilterdone}} of the
% function \ml+filter+ by:
\ref{ex/crible} 節のエラトステネスのふるいの例を Unix プロセスではなくスレッドを使って
動かすためには、 \ml+filter+ 関数の~{\ref{prog:sievefilterfork}--\ref{prog:sievefilterdone}} 行目を
%
\begin{codefile}{thcrible.ed}
f th/crible.ml
r finalcrible.ml
/fork/,/waitpid/c
\end{codefile}
%
\begin{listingcodefile}{thcrible.ed}
    let p = Thread.create filter (in_channel_of_descr fd_in) in
    let output = out_channel_of_descr fd_out in
    try
      while true do
        let n = input_int input in
        if List.exists (fun m -> n mod m = 0) first_primes then ()
        else output_int output n
      done;
    with End_of_file ->
      close_out output;
      Thread.join p
\end{listingcodefile}
%
% and the lines~{\ref{prog:sievefork}--\ref{prog:gen}} of the function
% \ml+sieve+ by:
に変え、さらに \ml+sieve+ 関数の~{\ref{prog:sievefork}--\ref{prog:gen}} 行目を
%
\begin{codefile}{thcrible.ed}
.
/fork/,/waitpid/c
\end{codefile}
%
\begin{listingcodefile}{thcrible.ed}
  let k = Thread.create filter (in_channel_of_descr fd_in) in
  let output = out_channel_of_descr fd_out in
  generate len output;
  close_out output;
  Thread.join k;;
\end{listingcodefile}
%
\begin{codefile}{thcrible.ed}
.
wq
\end{codefile}
%
% However, we cannot expect any significant gain from this example,
% which uses few processes relative to computation time.
に変えれば十分です。ただし、計算時間に比べてプロセスをあまり使わないこの例では
実行速度の高速化は望めません。
\end{example}

%% Pour attendre sur un seul descripteur de fichier (en écriture ou en
%% lecture), on peut utiliser l'une des deux versions spécialisée:
%% \begin{lstlisting}
%% val $\indexlibvalue{Thread}{wait_timed_read}$ : file_descr -> float -> bool
%% val $\indexlibvalue{Thread}{wait_timed_write}$ : file_descr -> float -> bool
%% \end{lstlisting}
%% On peut également attendre la terminaison d'un (vrai) processus fils
%% ou attendre sur un signal.
%% \begin{lstlisting}
%% val $\indexlibvalue{Thread}{wait_pid}$ : int -> int * process_status
%% val $\indexlibvalue{Thread}{wait_signal}$ : int -> int * process_status
%% \end{lstlisting}
%% Expliquer la différence entre la primitive \ml+wait\_pid+ et
%% \ml+Unix.waitpid+.

% \section{Synchronization among threads: locks}
\section{スレッドの同期: ロック}
% The functions in this section are defined in the
% \libmodule{Mutex} module (as in \ml+Mut+ual \ml+ex+clusion).\medskip
この節で説明される関数は \libmodule{Mutex} (\ml+Mut+ual \ml+ex+clusion) モジュールに定義されています。

% We mentioned above a problem of concurrent access to mutable
% resources. In particular, the following scenario illustrates the
% problem of access to shared resources.  Consider a counter $c$ and two
% processes $p$ and $q$, each incrementing the counter in parallel.
前の節で変更可能なリソースに対する並行アクセスの問題に触れました。
次のシナリオは共有リソースへのアクセスの問題を説明します。
二つのスレッド $p$ と $q$ がカウンタ $c$ の値を進めようとしているとします。

% Assume the scenario described in figure~\ref {fig/competition}.
% Thread $p$ reads the value of counter $c$, then gives control to $q$.
% In its turn, $q$ reads the value of $c$, then writes the value $k+1$
% to $c$.  The thread $p$ resumes control and writes the value $k+1$ to
% $c$. The final value of $c$ is therefore $k+1$ instead of $k+2$.
図\ref {fig/competition}にあるシナリオを考えます。
スレッド $p$ がカウンタ $c$ の値を読み、実行が $q$ へ移ります。
この次に $q$ は $c$ の値を読み、値 $k+1$ を $c$ に書き込みます。
次に実行は $p$ へと戻りますが、このスレッドは $k+1$ を $c$ に書き込みます。
この結果、最小的な $c$ の値は $k+2$ ではなく $k+1$ となってしまいます。
\begin{myfigure}
\begin{myimage}[width="100\%"]
\begin{tikzpicture}
\node at (-1,0.75) {スレッド$p$};
\node at (-1,0) {スレッド$q$};
\node at (-1,-1){$c$の値};

\node at (1,0.75) {$k$を読む};
\node at (1,-1) {$k$};

\node at (2.5,0.0) {$k$を読む};
\node at (2.5,-1) {$k$};

\node at (4.5,0) {$k+1$を書く};
\node at (4.5,-1) {$k+1$};

\node at (6.5,0.75) {$k+1$を書く};
\node at (6.5,-1) {$k+1$};

\draw[->] (-2.5, -0.5) -- (8.3, -0.5);

\node at (8.5, -0.5) [anchor=west]{time};
\end{tikzpicture}
\end{myimage}
\caption {共有リソースへのアクセスの競合}
\label{fig/competition}
\end{myfigure}

% This classic problem can be resolved by using locks that
% prevent arbitrary interleaving of $p$ and $q$.
この古典的な問題はロックを使って $p$ と $q$ が交互に実行されることをなくすことで解決できます。

% Locks are shared objects that can be held by at most a single thread
% within a program at a time. A lock is created by the function
% \ml+create+.
ロックとは共有オブジェクトであり、プログラムの各時点でロックを所有できるのは
多くとも一つのスレッドだけです。
ロックは \ml+create+ 関数で作成します。
%
\begin{codefile}{tmpmutex.mli}
  type t
\end{codefile}
%
\begin{listingcodefile}{tmpmutex.mli}
val $\libvalue{Mutex}{create}$ : unit -> t
\end{listingcodefile}
%
% This function returns a new lock, initially not held by any thread.
% To acquire an existing lock, it is necessary to use the system call
% \pthreadcall[mutex]{lock} with the lock as argument. If the lock is
% held by another thread, the caller is frozen until the lock is released.
% A lock must be released explicitly by the thread that holds it with
% the system call \pthreadcall[mutex]{unlock}.
この関数はどのスレッドにも所有されていない新しいロックを返します。
作成したロックを取得するには、そのロックに対してシステムコール \pthreadcall[mutex]{lock} を
呼ぶ必要があります。
目的のロックが他のスレッドによって所有されていた場合、 \ml+lock+ を読んだスレッドは
ロックが開放されるまでブロックします。
ロックを所有しているスレッドはシステムコール \pthreadcall[mutex]{unlock} によって
ロックを開放しなければいけません。
%
\begin{listingcodefile}{tmpmutex.mli}
val $\libvalue{Mutex}{lock}$ : t -> unit
val $\libvalue{Mutex}{unlock}$ : t -> unit
\end{listingcodefile}
%
% The \ml+lock+ call behaves like \ml+Thread.join+ with respect to
% signals: if the thread receives a signal while executing \ml+lock+,
% the signal will be noted ({\ie} the {\ocaml} runtime will be notified
% that the signal has arrived), but the thread will continue to wait so
% that \ml+lock+ effectively returns only when the lock has been
% acquired, and never raises the \ml+EINTR+ exception.  The real
% treatment of the signal by {\ocaml} will happen only upon the return
% from \ml+lock+.
シグナルへの動作について、\ml+lock+ は \ml+Thread.join+ に似ています。
もし \ml+lock+ の実行中にシグナルを受け取った場合、そのシグナルは記録され
\ocaml ランタイムがシグナルの到着を通知しますが、
関数を呼び出したスレッドは \ml+lock+ がロックが取得できるまでブロックされ、
\ml+lock+ から \ml+ENITR+ 例外が出ることはありません。
\ocaml によるシグナルの処理は \ml+lock+ が返るタイミングで行われます。

% We can also try to acquire a lock without blocking with the system call
% \pthreadcall[mutex]{trylock}
システムコール \pthreadcall[mutex]{trylock} を使うとブロックせずにロックの取得を試みることができます。
%
\begin{listingcodefile}{tmpmutex.mli}
val $\libvalue{Mutex}{try\_lock}$ : t -> bool
\end{listingcodefile}
%
% This function returns \ml+true+ if the lock has been acquired and
% \ml+false+ otherwise.  In the latter case, execution is not suspended
% since the lock is not acquired. The thread can therefore do something
% else and eventually return and try its luck later.
この関数はロックが取得できた場合に \ml+true+ を返し、それ以外のときには \ml+false+
を返します。後者の場合、ロックは取得されないのでスレッドの実行が停止することはありません。
そのためスレッドは他の処理を行ってからもう一度ロックの取得を試みることができます。

\begin{example}

% Incrementing a global counter used by several threads poses a
% synchronization problem: the instants between reading the value of the
% counter and writing the incremented value are in a critical region,
% {\ie} two threads cannot be in this region at the same time. The
% synchronization can easily be managed with a lock.
複数のスレッドによって利用されるグローバルなカウンタを進める処理には
同期の問題があります。
カウンタの値を読んでから新しい値を書き込むまでの間はクリティカルセクション、
つまり二つ以上のスレッドが同時に実行してはいけない区間になります。
ロックを使えば同期を簡単に管理できます。
%
\begin{listingcodefile}{cthread.ml}
type counter = { lock : Mutex.t; mutable counter : int }
let newcounter () = { lock = Mutex.create (); counter = 0 }
let addtocounter c k =
  Mutex.lock c.lock;
  c.counter <- c.counter + k;
  Mutex.unlock c.lock;;
\end{listingcodefile}
%
% The sole read operation on the counter poses no problem. It can be
% performed in parallel with a modification of the counter: the result
% will simply be the value of the counter just before or just after the
% modification, both results being consistent.
カウンタの値を読むだけであれば同期の問題は起こりません。
カウンタの変更と同時に読み込みが起こることはありえます。
しかしその場合書き込みの直前または直後の値を読み込むだけであり、
いずれにせよ矛盾が起きることはありません。
\end{example}

% A common pattern is to hold a lock temporarily during a function call.
% It is of course necessary to make sure to release the lock at the end
% of the call, whether the call succeeded or failed. We can abstract
% this behavior in a library function:
ある関数を実行する間ロックを保持するというパターンがよく必要になります。
このパターンでは関数が成功するか失敗するかにかかわらず最後にロックを
開放することが当然必要になります。
この処理を次のライブラリ関数に中傷します:
\begin{codefile}{misc.mli}
val run_with_lock : Mutex.t -> ('a -> 'b) -> 'a -> 'b
\end{codefile}
\begin{listingcodefile}{misc.ml}
let run_with_lock l f x =
  Mutex.lock l; try_finalize f x Mutex.unlock l
\end{listingcodefile}
% In the preceding example, we could also have written:
一つ前の例は次のように書けます:
\begin{codefile}{cthread.ml}
open Misc;;
\end{codefile}
\begin{listingcodefile}{cthread.ml}
let addtocounter c =
  Misc.run_with_lock c.lock (fun k -> c.counter <- c.counter + k)
\end{listingcodefile}

\begin{example}
% An alternative to the model of the server with threads is to start a
% number of threads in advance which handle requests in parallel.
スレッドを使ったサーバのモデルの代わりに、
リクエストを並列に処理するスレッドをいくつか事前に作っておくことができます。
%
\begin{codefile}{farm.mli}
open Unix
\end{codefile}
%
\begin{listingcodefile}{farm.mli}
val tcp_farm_server :
  int -> (file_descr -> file_descr * sockaddr -> 'a) -> sockaddr -> unit
\end{listingcodefile}
%
% The \ml+tcp_farm_server+ function behaves like \ml+tcp_server+ but
% takes an additional argument which is the number of threads to start,
% each of which will become a server at the same address. The advantage
% of a pool of threads is to reduce the time to handle each connection
% by eliminating the cost of creating a thread for it, since they are
% created once and for all.
\ml+tcp_farm_server+ 関数は \ml+tcp_server+ と同じように動作しますが、
開始するスレッドの数を表す引数を追加で取ります。
スレッドは同じアドレスに対するサーバになります。
スレッドプールを使う利点はスレッドを一括して事前に作っておくことで
接続を処理するときにスレッドを作る処理の分だけ時間を節約できる点です。
%
\begin{codefile}{farm.ml}
open Sys;;
open Unix;;
\end{codefile}
%
\begin{listingcodefile}{farm.ml}
let tcp_farm_server n treat_connection addr =
  let server_sock = Misc.install_tcp_server_socket addr in
  let mutex = Mutex.create () in
  let rec serve () =
    let client =
      Misc.run_with_lock mutex
        (Misc.restart_on_EINTR accept) server_sock in
    treat_connection server_sock client;
    serve () in
  for i = 1 to n-1 do ignore (Thread.create serve ()) done;
  serve ();;
\end{listingcodefile}
% The only precaution to take is to ensure mutual exclusion around the
% \ml+accept+ so that only one of the threads accepts a connection at a
% time. The idea is that the \ml+treat_connection+ function performs a
% sequential treatment, but it is not a requirement~---~we can
% effectively combine a pool of threads with the creation of new
% threads, which can be dynamically adjusted depending on the load.
唯一注意するべき点は \ml+accept+ 関数を排他的に実行することです。
これによって接続を受け付ける関数が常に多くとも一つとなります。
ここでは \ml+treat_connection+ 関数がシーケンシャルな処理を行うことを想定していますが、
これが必須なわけではありません。スレッドプールとスレッドの作成を
同時に行って使用率に応じてスレッドの数を調整することもできます。
\end{example}

% Acquisition of a lock is an inexpensive operation when it succeeds
% without blocking. It is generally implemented with a single
% \quotes{test-and-set} instruction provided by all modern processors
% (plus other small costs that are involved, such as updating caches).
% However, when the lock is not available, the process must be suspended
% and rescheduled later, which involves a significant additional cost.
% We must therefore incur this penalty only for a real suspension of a
% process in order to give control to another, and not for its potential
% suspension during the acquisition of a lock.  Consequently, we will
% almost always want to release a lock as soon as possible and take it
% back later if necessary, rather than simply holding onto the lock.
% Avoiding these two operations would have the effect of enlarging the
% critical region and therefore the frequency with which another thread
% finds itself effectively in competition for the lock and in need of
% suspension.
ロックの取得がブロックせずに成功した場合、時間のかからない処理となります。
ロックの取得は全ての現代的なプロセッサで提供される \quotes{test-and-set} 命令
(とキャシュの更新などのコストの小さないくつかの処理) で一般的に実装されます。
一方ロックが利用可能でない場合は、スレッドは実行を停止しもう一度スケジュールされるまで
待たなくてはならず、大量のコストがかさみます。
そのためこのコストを取るのは他のスレッドに実行を受け渡すために本当に実行を中断するときだけにするべきであり、
ロックを取得しようとした結果実行がブロックされたというのはできる限り避けるべきです。
したがってほとんど常にロックはなるべく早く開放して必要になったらもう一度取得するようにするべきで、
ロックを保持したままでいることは避けるべきです。
ロックの開放を行わないとクリティカルセクションが大きくなり、
他のスレッドのロックの取得が競合し、実行を停止しなければならなくなる頻度が上がります。

% Locks reduce interleaving.  In return, they increase the risk of
% deadlock.  For example, there is a deadlock if a thread $p$ waits for
% a lock $v$ held by a thread $q$ which itself waits for a lock $u$ held
% by $p$.  (In the worst case, a thread waits for a lock that it holds
% itself.)  Concurrent programming is difficult, and guarding against
% deadlock is not always easy.  A simple way of avoiding this situation
% that is often possible consists of defining a hierarchy among the
% locks and ensuring that the order in which the locks are acquired
% dynamically respects the hierarchy: a thread never acquires a lock
% unless that lock is dominated by all the other locks that the thread
% already holds.
ロックによってスレッドが交互に実行されなくなりますが、そのかわりデッドロックの
危険性があります。
例えばスレッド $p$ がロック $v$ の開放を待っていて、$v$ を持っているスレッド $q$ は
$p$ が持っているロック $u$ を待っている場合です (最悪な場合には、スレッドが自分が持っているロックを
待つこともあります) 。
並行プログラミングは難しく、デッドロックの排除は常に簡単なわけではありません。
デッドロックを避ける単純で実行可能なことが多い方法は、ロックに階層を定義して
ロックの取得が階層をたどるようにするというものです。
スレッドがあるロックを取得できるのはそのロックよりも下のロックを全て取得しているときだけです。

% \section{\label{ex/th-relais}Complete example: {\normalfont\http} relay}
\section{\label{ex/th-relais}完全な例: {\normalfont\http} リレー}

% We modify the {\http} relay developed in section~\ref{sec/relay} so
% that it services requests using threads.
\ref{sec/relay} 節で作成した \http リレーを改変し、リクエストを
スレッドを使って処理するようにします。

% Intuitively, it suffices to replace the \ml+establish_server+ function
% that creates a process clone with a function that creates a
% thread.  We must however take certain precautions.  The challenge
% with threads is that they share the entire memory space.  We must
% therefore ensure that the threads are not \quotes{stepping on each
% other's toes} with one undoing what was just done by another.
% That typically happens when two threads modify the same mutable
% structure in parallel.
直感的には、プロセスのクローンを作成する \ml+establish_server+ 関数を
スレッドを作るようにすればすみます。
しかし、いくつか注意点があります。
複数のスレッドを使用する上での困難はメモリ空間を全て共有することです。
そのためあるスレッド行ったことを他のスレッドが打ち消してしまうような、
\quotes{互いのつま先を踏み合う} ことのないようにしなければなりません。
これは二つのスレッドが変更可能な同じ構造体を並列に変更するときによく起こります。

% In the case of the {\http} server, there are several changes to make.
% Let us start by resolving problems with access to resources.  The
% \ml+proxy_service+ function, described in section~\ref{page/get_url},
% handles the treatment of connections.  Via the intermediary functions
% \ml+parse_host+, \ml+parse_url+ and \ml+parse_request+, it calls the
% \ml+regexp_match+ function which uses the \ml+Str+ library.  However,
% this library is not re-entrant (the result of the last search is
% stored in a global variable). This example shows that we must beware
% of calls to innocent-looking functions that hide potential collisions.
% In this case we will not rewrite the \ml+Str+ library but simply
% sequentialize its use.  It suffices to protect calls to this library
% with locks (and there is really no other choice). We must still take
% the precaution of releasing the lock when returning from the function
% abnormally due to an exception.
\http サーバにはいくつかの変更が必要になります。
まずリソースへのアクセスの問題から始めます。
\ref{page/get_url} 節で説明した \ml+proxy_service+ 関数は接続への対応を処理します。
\ml+parse_host+ と \ml+parse_url+ そして \ml+parse_request+ を途中で使って、
\ml+proxy_service+ は \ml+Str+ ライブラリの \ml+regexp_match+ 関数を呼びます。
しかし、このライブラリは再入可能ではありません (最後の探索の結果がグローバル変数に
か格納されます)。
一見害の無いように見える関数呼び出しも内部にデータの衝突を隠しているかもしれないので
注意しなければいけないことをこの例は示しています。
今回は \ml+Str+ ライブラリを書き直すのではなく単純にその利用をシーケンシャルにします。
ライブラリ関数の呼び出しをロックで防御するだけで十分です (他の選択肢があるわけでもありません)。
例外によって関数が異常終了した場合にもロックを開放するように注意する必要があります。

% To modify the existing code as little as possible, we can just rename
% the definition of \ml+regexp_match+ in the \ml+Url+ module as
% \ml+unsafe_regexp_match+ and then define \ml+regexp_match+ as a
% protected version of \ml+unsafe_regexp_match+.
現在のコードへの変更を最小限にするために、\ml+Url+ モジュールの
\ml+regexp_match+ 関数の名前を \ml+unsafe_regexp_match+ に変更し、
新たに \ml+regexp_match+ 関数を防御された \ml+unsafe_regexp_match+ として作成します。
%
\begin{codefile}{thurl.ed}
f th/url.ml
r url.ml
/^let regexp_match/s/regexp_match/unsafe_regexp_match/
/^let/-1a
open Misc;;
\end{codefile}
%
\begin{listingcodefile}{thurl.ed}
let strlock = Mutex.create ();;
let regexp_match r string =
  Misc.run_with_lock strlock (unsafe_regexp_match r) string;;
\end{listingcodefile}

% The change is rather minimal.  It should be noted that the
% \ml+regexp_match+ function includes both the expression matching and
% the extraction of the matched groups.  It would definitely have been
% incorrect to protect the \ml+Str.string_match+ and
% \ml+Str.matched_group+ functions individually.
変更は最小限だと言えるでしょう。
\ml+regexp_match+ 関数が正規表現に対するマッチングとマッチしてグループの
取得を両方を含むことを指摘しておくべきでしょう。
\ml+Str.string_match+ と \ml+Str.matched_group+ を個別にロックで防御するのは
完全な間違いです。

% Another solution would be to rewrite the analysis functions without using
% the \ml+Str+ library.  But there is no reason for such a choice, since
% synchronizing the library primitives is easy to do and does not turn
% out to be a source of inefficiency.  Obviously, a better solution
% would be for the \ml+Str+ library to be re-entrant in the first place.
もう一つの解法は \ml+Str+ ライブラリを使わずに解析関数を書き直すことです。
しかしライブラリ関数を同期して実行するのは簡単であり、実行を遅くしないことが
分かっているので、この方法を取るべき理由は何もありません。
また \ml+Str+ が最初から再入可能であればそのほうが良かったでしょう。

% The other functions that are called are already re-entrant, in particular
% the \ml+Misc.retransmit+ function that allocates different buffers for
% each call.
他の関数はすでに再入可能です。
例えば \ml+Misc.retransmit+ 関数は呼び出しごとに違うバッファを確保します。

% However, there are still some precautions to take regarding error
% handling.  The handling of a connection by a thread must be robust, as
% explained above. In particular, in case of error, the other threads
% must not be affected. In other words, the thread must terminate
% \quotes{normally}, properly closing the connection in question and
% going back to accepting other pending connections.  We must first of
% all replace the call to \ml+exit+ in \ml+handle_error+ because it is
% essential not to kill the whole process.  A call to \ml+Thread.exit+
% would not be correct either, because thread termination does not close
% its (shared) descriptors, the way the system does for process
% termination.  An error in the handling of a connection would leave the
% connection open.  The solution consists of raising an \ml+Exit+
% exception that allows the finalization code to do what is required.
% We must now protect \ml+treat_connection+ by catching all errors, in
% particular \ml+Exit+ but also \ml+EPIPE+, which can be raised if the
% client closes the connection prematurely.  We will take care of this
% by using a protected function.
しかしエラー処理に関連して気をつけなければいけないことがあります。
前に書いたように、スレッドによる接続の処理はロバストでなければいけません。
特にエラーが起こった場合に他のスレッドが影響を受けてはいけません。
つまりそのスレッドは受け持った接続を適切に閉じて
他の待っている接続を受け付ける状態に戻るという \quotes{通常} 終了をする必要があります。

このためにまず \ml+handle_error+ 関数中の \ml+exit+ の呼び出しを \ml+Thread.exit+ に変更します。
これは \ml+exit+ がプロセス全体を終了させるためです。
しかしただ \ml+Thread.exit+ を読ぶだけでは正しくなりません。なぜなら
プロセスの終了と違ってスレッドの終了は (共有された) ディスクリプタを閉じないからです。
接続処理中にエラーが起きても接続は開いたままなので、 \ml+Exit+ 例外を出して最終処理を行うようにする
必要があります。そして \ml+treat_connection+ 関数は \ml+EPIIE+ だけでなく \ml+Exit+ を含んだ全ての
例外を補足しなければいけません。関数を例外を捕捉用に呼び出す関数を使ってこれを行います:
%
\begin{codefile}{thurl.ed}
.
1;#
/handle_error/;#
-1a
exception Exit;;
.
.,++s/exit 2/raise Exit/
wq
\end{codefile}
%
\begin{codefile}{thproxy.ed}
f th/proxy.ml
r proxy.ml
/let proxy/-1a
\end{codefile}
\begin{listingcodefile}{thproxy.ed}
let allow_connection_errors f s =
  try f s with Exit | Unix_error(EPIPE,_,_) -> ()
\end{listingcodefile}
%
\begin{codefile}{thproxy.ed}
.
/let treat_connection/c
\end{codefile}
%
\begin{listingcodefile}{thproxy.ed}
let treat_connection s =
  Misc.co_treatment s (allow_connection_errors proxy_service) in
\end{listingcodefile}
%
\begin{codefile}{thproxy.ed}
.
wq
\end{codefile}

\begin{exercise}[noanswer]
% Rewrite the proxy for the {\http}/1.1 protocol using threads.
\http/1.1 プロトコルのプロキシをスレッドを使って書き直してください。
\end{exercise}

\begin{exercise}[noanswer]
% Coroutines can be seen as a very particular kind of threads where each
% process must surrender control explicitly before another can execute.
% Give an implementation of coroutines using threads.
コルーチンは別のスレッドへの実行の受け渡しを常に明示的に行う必要がある
非常に特殊なスレッドと見ることができます。
スレッドを使ってコルーチンを実装してください。
\end{exercise}

% \section{Conditions}
\section{条件変数}

% The functions described in this section are defined in the
% \libmodule{Condition} module.
この節で説明される関数は \ml+Condition+ モジュールに定義されています。

% Synchronization with locks is very simple, but it is not sufficient:
% locks allow waiting for shared data to be free, but do not allow
% waiting for the data to have a particular state.  Let us replace the
% example of a counter by a (first-in/first-out) queue shared among
% several threads.  Adding a value to the queue can be synchronized by
% using a lock as above, since no matter what the state of the queue,
% we can always add an element.  But what about removing an element
% from the queue?  What should be done when the queue is empty?  We
% cannot hold the lock while waiting for the queue to be filled, because
% that would completely prevent another thread from filling the queue.
% So it must be released.  But how can we know when the queue is no
% longer empty, except by testing it periodically?  This solution,
% called \quotes{busy-waiting}, is definitely not satisfactory.  Either
% it consumes computing cycles unnecessarily (period too short) or else
% it it is not reactive enough (period too long).
ロックを使った同期はとてもシンプルですが、機能が十分ではありません。
ロックを使うと共有データが開放されるのを待つことができますが、
データがある状態になるのを待つことはできません。

カウンタの例をスレッド感で共有される (先入れ先出しの) キューを使って書き直してみましょう。
キューに要素を追加することはこれまで説明してきたロックによって行えます。これは
キューの状態にかかわらず要素を追加することは常に可能なためです。

しかしキューから要素を削除する場合はどうでしょうか?
キューが空のときは何をすればいいでしょうか?
キューが空の場合にロックを持ちながら新しい要素を待つことはできません。
なぜならキューに要素を追加するにはロックが必要なために、ロックを持っている限り
キューに新しい要素が足されることはないからです。
だとすれば、キューが空でないことをどうやって知ることができるでしょうか?
キューの状態を定期的にチェックする方法は \quotes{ビジーウェイト} と呼ばれ、望ましい解決法ではありません。
(周期が短ければ) 計算機のサイクルを無駄遣いし、 (周期がながければ) 反応時間が長くなるからです。

% \emph{Conditions} provide a solution to this problem.  A thread that
% holds a lock can wait for a condition object until another thread
% sends a signal on that condition.  As with locks, conditions are
% passive structures that can be manipulated by synchronization
% functions.  They can be created by the \ml+create+
% function.
\emph{条件変数} がこの問題を解決します。
ロックを持つスレッドは条件変数に対して別のスレッドがシグナルを送るまで待つことができます。
ロックと同じように、条件変数は同期関数によって生成される中身の見えないオブジェクトです。
\ml+create+ 関数で条件変数を作ることができます。
%
\begin{codefile}{tmpcondition.mli}
type t
\end{codefile}
%
\begin{listingcodefile}{tmpcondition.mli}
val $\libvalue{Condition}{create}$ : unit -> t
\end{listingcodefile}
%
% A process $p$ that \emph{already holds} a lock \ml+v+ can wait on a
% condition \ml+c+ and the lock \ml+v+ with the system call \pthreadcall[cond]{wait}.
% The process $p$ informs the system that it is waiting on the condition
% \ml+c+ and the lock \ml+v+, then releases the lock \ml+v+ and goes to
% sleep.  It will not be woken up by the system until another thread $q$
% signals a change on the condition \ml+c+ and the lock \ml+v+ is
% available; the process $p$ will then hold the lock \ml+v+ again.
システムコール \pthreadcall[cond]{wait} を使うと、
ロック $v$ を \emph{すでに持つ} プロセス $p$ は条件変数 \ml+c+ とロック \ml+v+
に対して待つことができます。
この呼び出しによって プロセス $p$ はシステムに条件変数 \ml+c+ とロック \ml+v+ に対して待っていると伝え、
ロック \ml+v+ を開放して実行を中断します。
他のスレッド $q$ が条件変数 \ml+c+ の変更をシグナルし、かつロック \ml+v+ が
利用可能になると \ml+v+ を持った状態で実行が再開されます。
%
\begin{listingcodefile}{tmpcondition.mli}
val $\libvalue{Condition}{wait}$ : t -> Mutex.t -> unit
\end{listingcodefile}
%
% Note: it is an error to call \ml+wait c v+ without holding the lock
% \ml+v+.  The behavior of \ml+wait c v+ with respect to signals is the
% same as for \ml+Mutex.lock+.
ロック \ml+v+ を取得していない状態で \ml+wait c v+ を呼ぶとエラーとなります。
またシグナルに対する \ml+wait c v+ の動作は \ml+Mutex.lock+ を同じです。

% When a thread signals a change on a condition, it can either ask for
% all threads waiting on that condition to be woken up
% (system call \pthreadcall[cond]{broadcast}), or else for just one of them to be
% woken up (system call \pthreadcall[cond]{signal}).
スレッドが条件変数が変更されたシグナルを送ると、
その条件変数に対して待っている全てのスレッドを起動する (システムコール \pthreadcall[cond]{broadcast}) か、
それらのうち一つを起動する (システムコール \pthreadcall[cond]{signal}) かのどちらかが行われます。
%
\begin{listingcodefile}{tmpcondition.mli}
val $\libvalue{Condition}{signal}$ : t -> unit
val $\libvalue{Condition}{broadcast}$ : t -> unit
\end{listingcodefile}
%
% Sending a signal or a broadcast on a condition does not require
% holding a lock (unlike waiting), in the sense that it will not trigger
% a \quotes{system} error.
条件変数のシグナルの送信またはブロードキャストは (\ml+wait+ と違って) ロックを必要と
しません。これによってシステムのエラーは起きませんが、それでもプログラミング上のエラーは起きます。
%%
%% Est-ce vrai? c'est à celui qui se met en attente à avoir pris le verrou
%% avant de tester, puis fait wait. Il est donc sûr que la condition n'est
%% pas vrai à ce moment là.
%%
% However, it can sometimes be a programming error.
%% En effet, émettre un signal sans posséder le verrou sur
%% lequel les récepteurs de ce signal sont en attente signifie que
%% l'émission du signal n'est pas synchronisée avec la mise en attente et
%% peut donc se produire avant la mise en attente qu'il est supposé
%% interrompre: le signal sera alors ignoré et l'attente jamais
%% interrompue (voir l'exemple des queues ci-dessous).

% The choice between waking up one thread or all the threads depends on
% the problem.  To consider the example of the queue again, if a thread
% adds an element to an empty queue, there is no need to wake up all the
% others, since only one will effectively be able to remove that
% element.  On the other hand, if it adds a number of elements that is
% either not statically known or very large, it must wake up all the
% threads.  Note that if adding an element to a non-empty queue does not
% send a signal, then adding an element to an empty queue must send a
% broadcast, since it could be followed immediately by another addition
% (without a signal) and therefore behave like a multiple addition.
% In summary, either send a signal on every addition, or send a
% broadcast only when adding to an empty queue.
% The choice between these two strategies is a bet on whether the queue
% is usually empty (first solution) or usually non-empty (second
% solution).
スレッドを一つだけ起動するのか全て起動するかは問題によります。
キューの例をもう一度考えると、スレッドが空のキューに要素を入れたときに
待っているスレッドを全て起動する必要はありません。
キューから要素を取り出して処理を行えるのは一つのスレッドだけだからです。
一方、もしスレッドが複数の要素をキューに入れ、その要素の数が静的にわからない場合には
待っている全てのスレッドを起動する必要があります。

空でないキューに要素を追加するときにシグナルを出さない場合には、
空のキューへの要素の追加は常にブロードキャストを行う必要があることに注意してください。
空のキューへの要素の追加されてからシグナルが発せられるまでに他の要素が追加されることがあり、
このときは要素が複数追加されたとして対処しなければならないためです。

% Often, one thread knows only an approximation of the reason why
% another thread is waiting on a condition.  It will therefore signal
% the condition whenever the situation \emph{might} be what the other
% thread is waiting for.  An awakened thread, therefore, cannot assume
% that the condition it was waiting is now satisfied.  It must, in
% general, re-test the state of its shared data, and if necessary wait
% on the condition again.  This does not constitute busy-waiting, because
% it only happens when another thread signals the condition.
通常スレッドは他のスレッドが条件変数に対して待っている理由を近似的にしか知りません。
そのため他のスレッドが条件変数を待っているかもしれない状況では常にシグナルすることが
求められます。また起動されたスレッドは期待した状況が整っていると仮定してはいけません。
一般的には、起動されたスレッドは共有状態のデータを読み取って所望の条件が成り立っていることを確認し、
必要ならばもう一度待つ必要があります。
この処理は他のスレッドがシグナルしたときにだけ起こるのでビジーウェイトとは違います。

% Here is another justification for this approach: when a thread has
% just produced a lot of some resource and wakes all the others using a
% \ml+broadcast+, nothing prevents the first one that wakes up from
% being greedy and exhausting the entire resource.  The second one to
% wake up must go back to sleep, hoping to be luckier next time.
このアプローチを正当化する理由がもう一つあります。
スレッドが大量のリソースを生み出して \ml+broadcast+ を使って全てのスレッドを
起動したときに、最初に実行されるスレッドは貪欲に全てのリソースを消費することが可能な点です。
このとき二番目以降に起動されるスレッドは次回はもっとラッキーになるように願いながらもう一度
実行を中断することになります。

% We can now give a concrete solution for shared queues.  The queue
% structure defined in the \libmodule{Queue} module is extended with a lock and
% a \ml+non_empty+ condition.
今までの説明でキューに関する完全な回答の準備が整いました。
\libmodule{Queue} に定義されているキューの構造がロックと条件変数 \ml+non_empty+ で
拡張されます。
%
\begin{listingcodefile}[style=numbers]{thcheck.ml}
type 'a t =
  { queue : 'a Queue.t; lock : Mutex.t; non_empty : Condition.t }
let create () =
  { queue = Queue.create ();
    lock = Mutex.create (); non_empty = Condition.create () }

let add e q =
  Mutex.lock q.lock;
  if Queue.length q.queue = 0 then Condition.broadcast q.non_empty;$\label{prog:broadcast}$
  Queue.add e q.queue;
  Mutex.unlock q.lock;;

let take q =
  Mutex.lock q.lock;
  while Queue.length q.queue = 0 $\label{prog:lock}$
  do Condition.wait q.non_empty q.lock done;  $\label{prog:slock}$
  let x = Queue.take q.queue in
  Mutex.unlock q.lock; x;;
\end{listingcodefile}
%
% Addition never blocks, but we must not forget to signal
% the \ml+non_empty+ condition when the list is empty beforehand,
% because it is possible that someone is waiting on the condition.
要素の追加ではスレッドはブロックしませんが、追加の前にキューが空だった場合に
条件変数 \ml+non_empty+ をシグナルすることを忘れてはいけません。
この条件変数に対して待っているスレッドが存在する可能性があるからです。

% Removal is a little more complicated: after acquiring the lock, we
% must try to remove an element from the queue. If the queue is empty,
% we must wait on the \ml+non_empty+ condition.  When awakened, we try
% again, knowing that we already have the lock.
要素の削除はもう少し複雑です。
スレッドはロックを取得してから削除を試みますが、
キューが空だった場合には条件変数 \ml+non_empty+に対して待ち
起動されたあとにもう一度同じことを行います。
同じことができるのは起動されたスレッドがロックを持っているからです。

% As explained above, the \ml+broadcast q.non_empty+ signal
% (line~\ref{prog:broadcast}) is executed by a thread $p$ already in
% possession of the lock \ml+q.lock+.
% This implies that a reader thread $q$ executing the \ml+take+ function
% cannot be between line~\ref{prog:lock} and~\ref{prog:slock}
% where it would have verified that the queue is empty but not yet have
% gone to sleep.  In this case, the signal sent by $p$ would be
% ineffective and ignored, since $q$ has not gone to sleep yet; but $q$
% would then go to sleep and not be woken up, because $p$ has already
% sent its signal.
% The lock therefore guarantees that either $q$ is already asleep or
% else has not yet tested the state of the queue.
上で説明したように、\ml+broadcast q.non_empty+ によるシグナル
(\ref{prog:broadcast} 行目) はすでにロック \ml+q.lock+ を持っているスレッド
$p$ によって実行されます。
これは読み込むを行うために \ml+take+ を実行するスレッド $q$ が
~\ref{prog:lock} 行目と~\ref{prog:slock} 行目の間の、
キューが空であることは確認したがまだ実行を中断していない状態にないことを意味します。
もしスレッド $q$ がこの状態になったとすると、$p$ によって送られるシグナルは無視され効果を持ちません。
その後 $q$ は実行を中断しますが、 $p$ がすでにシグナルしているために $q$ が起動されることはありません。
スレッド $p$ によるロックの取得は $q$ がすでに実行を中断したかキューの状態を調べていないかのどちらかで
あり、この状況にならないことことを保証します。

\begin{exercise}
% Implement a variant in which the queue is bounded: addition to the
% queue becomes blocking when the size of the queue reaches a fixed
% value. (In a concurrent world, we might need this scheme to avoid
% having a producer that produces endlessly while the consumer is
% blocked.)
有界なキューを実装してください。
キューへ追加されるのはキューがある長さに達したときにブロックする機能です
(並行プログラミングで限りなく生産する生産者と実行が
ブロックする消費者が存在する場合にこのキューが必要になります)。
\end{exercise}
\begin{answer}
% We must introduce an additional \ml+non_full+ condition.
% We also add a \ml+size+ field to allow queues of different sizes.
新しい条件変数 \ml+non_full+ と \ml+size+ フィールドを追加します。
%
\begin{listingcodefile}{thtqueue.ml}
type 'a t =
    { queue : 'a Queue.t; size : int; lock : Mutex.t;
      non_empty : Condition.t; non_full : Condition.t; }

let create k =
  if  k > 0 then
    { queue = Queue.create (); size = k; lock = Mutex.create ();
      non_empty = Condition.create (); non_full = Condition.create () }
  else failwith "Tqueue.create: empty size";;
\end{listingcodefile}
%
% Addition is a combination of the preceding versions of the
% \ml+add+ and \ml+take+ functions above.
要素の追加は \ml+add+ と \ml+take+ の処理を組み合わせたものになります。
%
\begin{listingcodefile}{thtqueue.ml}
let add x q =
  Mutex.lock q.lock;
  while Queue.length q.queue = q.size
  do Condition.wait q.non_full q.lock done;
  if Queue.is_empty q.queue then Condition.broadcast q.non_empty;
  Queue.add q x;
  Mutex.unlock q.lock;;
\end{listingcodefile}
%
% Removal is symmetric to addition (and must now signal \ml+non_full+
% when the queue is full beforehand), and is left to the reader.
要素の削除は追加と似ていますが、削除する前のキューが満杯な場合には \ml+non_full+ を
シグナルする必要があります。
%
\begin{codefile}{thtqueue.ml}
let take q =
  Mutex.lock q.lock;
  while Queue.length q.queue = 0
  do Condition.wait q.non_empty q.lock done;
  if Queue.length q.queue = q.size then Condition.broadcast q.non_full;
  let x = Queue.take q.queue in
  Mutex.unlock q.lock; x;;
\end{codefile}
%
% We get the behavior of unbounded queues by choosing \ml+max_int+ for
% \ml+size+.
\ml+size+ を \ml+max_int+ とすれば動作が制限のないキューと同じになります。
\end{answer}



% \section{Event-based synchronous communication}
\section{イベントベースの同期通信}

% The functions described in this section are defined in the
% \libmodule{Event} module.
この節で説明される関数は \ml+Event+ モジュールに定義されています。

% Locks and conditions together allow all forms of synchronization to be
% expressed.  However, their use is not always easy, as shown
% by the example of the initially simple queue whose synchronization
% code subsequently turned out to be subtle.
ロックと条件変数を使えばどんな形の同期も行うことができますが、
必ずしも簡単に行えるというわけではありません。
最初は単純だったキューでも同期のためのコードが緻密になることは
前節の例で見ました。

% Event-based synchronous communication is a collection of higher-level
% communication primitives that tend to facilitate concurrent
% programming.  The primitives in the \ml+Event+ module were initially
% developed by John Reppy as an extension of the \emph{Standard ML}
% language called \emph{Concurrent ML}~\cite{CML}.  In {\ocaml}, these
% primitives are located above the more elementary synchronization of
% locks and conditions.
イベントベースの同期通信は高レベルの通信プリミティブを集めたものであり、
並行プログラミングを容易にすることを意図しています。
\ml+Event+ モジュールのプリミティブはまず John Reppy によって \emph{Standard ML} 言語の拡張
\emph{Concurrent ML}~\cite{CML} として開発されました。
\ocaml ではこれらの通信プリミティブはロックと条件変数などのよりも基本的な同期の
上に位置しています。

% Communication occurs by sending \emph{events} along \emph{channels}.
% Channels are like \quotes{lightweight pipes}: they allow communication
% among threads in the same program and take care of synchronization
% between producers and consumers.  A channel carrying values of type
% \ml+'a+ has the type \ml+'a Event.channel+.  Channels are homogeneous
% and therefore always carry values of the same type.  A channel is
% created with the \ml+new_channel+ function.
通信は \emph{イベント} を \emph{チャンネル} を通じて送ることで行います。
チャンネルとは \quotes{軽量パイプ} のようなものです。
これを使うと同じプログラム内のスレッド間で通信を行い、生産者と消費者の間で
同期を行うことができます。\ml+'a+ 型の値を運ぶチャンネルは \ml+'a channel+ 型を
持ちます。チャンネルは途中で型を変えることなく常に同じ型の値を運びます。
チャンネルは \ml+new_channel+ 関数で作成されます。
%
\begin{codefile}{tmpevent.mli}
type 'a channel
type +'a event
\end{codefile}
%
\begin{listingcodefile}{tmpevent.mli}
val $\indexlibvalue{Event}{new\_channel}$ : unit -> 'a channel
\end{listingcodefile}
%
% Sending or receiving a message is not done directly, but through the
% intermediary of an event.  An elementary event is \quotes{sending a
% message} or \quotes{receiving a message}.  They are constructed by
% means of the following primitives:
メッセージの送受信は直接的にではなくイベントという中間体を通して行われます。
基本的なイベントは \quotes{メッセージの送信} と \quotes{メッセージの受信} であり、
以下のプリミティブによって作成できます。
%
\begin{listingcodefile}{tmpevent.mli}
val $\indexlibvalue{Event}{send}$ : 'a channel -> 'a -> unit event
val $\indexlibvalue{Event}{receive}$ : 'a channel -> 'a event
\end{listingcodefile}
%
% Construction of a message does not have an immediate effect: it just
% creates a data structure describing the action to be done.  To make an
% event happen, the thread must synchronize with another thread wishing
% to make the complementary event happen.  The \ml+sync+ primitive
% allows a thread to wait for the occurrence of the event passed
% as argument.
メッセージの作成してもすぐに何かが起こるわけではありません。
行われるべき動作を示すデータ構造が作られるだけです。
イベントを実際に起こすには、スレッドはそのイベントを実行すべき
スレッドと同期しなければいけません。
\ml+sync+ プリミティブを使うと引数として渡すイベントが実際に実行されるのを待つ
ことができます。
%
\begin{listingcodefile}{tmpevent.mli}
val $\indexlibvalue{Event}{sync}$ : 'a event -> 'a
\end{listingcodefile}
%
% Thus, to send a value \ml+v+ on the channel \ml+c+, we can execute
% \ml+sync (send c v)+.  The thread is suspended until the event occurs,
% that is to say until another thread is ready to receive a value on the
% channel \ml+c+.  In a symmetric fashion, a thread can wait for a
% message on channel \ml+c+ by performing \ml+sync (receive c)+.
まとめると、値 \ml+v+ をチャンネル \ml+c+ に送るには \ml+sync (send c v)+ を実行します。
\ml+sync+ を読んだスレッドはイベントが起こるまで、つまり他のスレッドがチャンネル \ml+c+
から値を受信可能になるまでブロックされます。
対称的に、スレッドがチャンネル \ml+c+ に対するメッセージを待つには
\ml+sync (recieve c)+ を使います。

% There is a competition among all the producers on one hand and all the
% consumers on the other.  For example, if several threads try to send a
% message on a channel but only one is ready to read it, it is clear
% that only one producer will make the event occur.  The others will
% remain suspended, without even noticing that another was
% \quotes{served} ahead of them.
全ての生産者と全ての消費者は競合します。
例えばいくつかのスレッドが一つのチャンネルにメッセージを送り
メッセージを受信できるスレッドが一つしか無かった場合、
自分の送ったイベントが起きる生産者は一つだけなのは明らかです。
他の生産者は実行を中断されたままであり、
先に他のスレッドがイベントを受け取ったことに気がつくことはありません。

% The competition can also occur within the same thread.
% Multiple events can be combined by the \ml+choose+ primitive.
競合は同じスレッド内でも起こりえます。
複数のイベントは \ml+choose+ プリミティブを使うとまとめることができます。
%
\begin{listingcodefile}{tmpevent.mli}
val $\indexlibvalue{Event}{choose}$ : 'a event list -> 'a event
\end{listingcodefile}
%
% The resulting event is an offer, in parallel, of the events passed as
% arguments, and occurs when exactly one of them occurs.  We distinguish
% between the offer of an event and its occurrence.  The call
% \ml+sync (choose [e1; e2])+ synchronizes by offering a choice of two events
% \ml+e1+ and \ml+e2+, but only one of the two events will effectively
% occur (the offer of the other event will be simultaneously canceled).
% The \ml+wrap_abort+ primitive allows to handle an event being
% canceled.
この関数は引数に渡されたイベントを並列にオファーし、これらのイベントのうちちょうど一つが
実行されます。ここではオファーと実行を区別しています。
\ml+sync (choose [e1; e2])+ は二つのイベント \ml+e1+ と \ml+e2+ の選択をオファー
するので、同期したときには二つのうち一つのイベントしか実行されません (同時にもう一つのイベントは
キャンセルされます)。
\ml+wrap_abort+ プリミティブを使うとイベントがキャンセルされたときの処理を設定できます。
%
\begin{listingcodefile}{tmpevent.mli}
val $\indexlibvalue{Event}{wrap\_abort}$ : 'a event -> (unit -> unit) -> 'a event
\end{listingcodefile}
%
% The call \ml+wrap_abort e f+ creates an event that is equivalent to
% \ml+e+, but if it is not chosen during synchronization, then the
% function \ml+f+ is executed.  (This is only interesting when it is
% part of a complex event.)
\ml+wrap_abort e f+ は \ml+e+ と同じイベントを作成しますが、
同期したときにこのイベントが実行されなかった場合には \ml+f+ が実行されます
(このイベントが複雑なイベントの一部であるときにだけ意味を持ちます)。

% A thread can try to synchronize on an event without blocking (somewhat
% like \ml+Mutex.try_lock+) with \ml+poll+.
\ml+poll+ を使うと (\ml+Mutex.try_lock+ のように) ブロックせずにイベントの
同期を試みることができます。
%
\begin{listingcodefile}{tmpevent.mli}
val $\indexlibvalue{Event}{poll}$ : 'a event -> 'a option
\end{listingcodefile}
%
% The call \ml+poll e+ offers the event \ml+e+ but if it cannot occur
% immediately, it cancels the offer rather than blocking and has no
% effect (or more exactly, behaves as if the expression \ml+poll e+ had
% been replaced by the value \ml+None+).  By contrast, if the event can
% happen immediately, then it behaves as if the thread had done
% \ml+sync e+, except that the value \ml+Some v+ is returned
% rather than \ml+v+.
\ml+poll e+ はイベント \ml+e+ をオファーしますが、このイベントがすぐに実行されない
場合にはブロックせずにオファーをキャンセルします。このとき関数呼び出しは
なんの効果も持ちません (より正確には、\ml+poll e+ という式が \ml+None+ で
置き換えられたかのような動作をします)。これに対して、
もしイベントがすぐに実行できるならば \ml+sync e+ が実行されたときの動作をます。
ただしその時の返り値は \ml+v+ ではなく \ml+Some v+ となります。

\begin{example}
% In section~\ref{ex/crible-copro} the example of the Sieve of Eratosthenes,
% the communication between different threads is done with pipes as in
% the original program, using system memory (the pipe) as intermediary.
% We may think that it is more efficient to communicate
% directly by using the memory of the process.  A simple solution
% consists of replacing the pipe by a channel on which integers are sent.
\ref{ex/crible-copro} 節で示したエラトステネスのふるいの例では、
異なるスレッド感の通信には元のプログラムと同じパイプが使われ、
システムメモリ (パイプ) が中間体でした。
プロセスのメモリを使って直接通信を行ったほうが効率的な可能性もあります。
単純な解法はパイプを整数が送られるチャンネルで置き換えることです。

% Sending integers on the channel is not sufficient, because we must
% also be able to detect the end of the stream.  The simplest is
% therefore to pass elements of the form \ml+Some n+ and to terminate by
% sending the value \ml+None+.  To minimize the changes, we use the code
% of the example in section~\ref{ex/crible}.  We simulate pipes and the
% functions for reading and writing pipes by channels and functions for
% reading and writing channels.
整数をチャンネルに送るだけでは十分ではなく、ストリームの終端を検出する必要もあります。
最も単純な解法は整数を \ml+Some n+ の形で送り、\ml+None+ を送ることで終了することです。
変更を最小にするために、\ref{ex/crible} 節の例で使ったコードを再利用します。
パイプとその入出力関数をチャンネルとその入出力関数でシミュレートします。

% It is sufficient to take the previous version of the program and
% change the input/output functions to ones that read and write a channel,
% rather than an input/output buffer from the \ml+Pervasives+ library.
% For example, we can insert the following code at the beginning of the
% program just after the \ml+open Unix;;+ directive:
前のバージョンのプログラムを取ってきて
入出力関数を \ml+Pervasives+ ライブラリの入出力バッファを使った
ものからチャンネルを使ったものに入れ替えればすみます。
例えば次のコードを \ml+open Unix+ 命令の後に追加します。
%
\begin{codefile}{theventcrible.ed}
f th/eventcrible.ml
r th/crible.ml
/let input_int/d
/let output_int/d
/generate/-1a
\end{codefile}
%
\begin{listingcodefile}{theventcrible.ed}
let pipe () = let c = Event.new_channel () in c, c
let out_channel_of_descr x = x
let in_channel_of_descr x = x

let input_int chan =
  match Event.sync (Event.receive chan) with
  | Some v -> v
  | None -> raise End_of_file
let output_int chan x = Event.sync (Event.send chan (Some x))
let close_out chan = Event.sync (Event.send chan None);;
\end{listingcodefile}
%
\begin{codefile}{theventcrible.ed}
.
wq
\end{codefile}

% However, if we compare the efficiency of this version with the
% previous one, we find that it is twice as slow.  Communication of each
% integer requires a synchronization between two threads and therefore
% several system calls for acquiring and releasing locks.  On the other
% hand, communication via pipes uses buffered \io{} that allows several
% thousand integers to be exchanged with each system call.
しかしチャンネルを使ったバージョンの実行速度を前のバージョンと比べると、
チャンネルを使うほうが二倍遅いことがわかります。
整数を通信するには二つのスレッドの同期が必要となり、
ロックの取得と開放に何回かシステムコールが必要となるためです。
一方で、バッファされた \io を使うパイプを用いた通信では
一回のシステムコールで数千個の整数がまとめて送られます。

% To be fair, one should also provide buffered communication on
% channels, using the channel only to exchange a packet of integers.
% The child can accumulate the results in a private queue, to which it
% can therefore write without synchronization.  When the queue is full,
% or upon an explicit request, it is emptied by synchronizing on the
% channel.  The parent has its own queue that it receives by
% synchronizing and empties gradually.
公平な比較のためには、チャンネルを使った通信にバッファを用意して
いくつかの整数をまとめてチャンネルを利用するようにするべきです。
子はプライベートなキューに計算結果を他のスレッドと同期することなく蓄積します。
キューが満杯になるか明示的な要求が届くと、
チャンネルが同期されてキューが空になります。
親は自分用のキューを持ち、このキューはチャンネル同期によって満たされ
計算が進むに連れて一つづつ消費されます。

% Here is a solution:
解は以下のようになります:
%
\begin{codefile}{thbuffercrible.ed}
f th/buffercrible.ml
r th/crible.ml
/generate/-1a
\end{codefile}
%
\begin{listingcodefile}{thbuffercrible.ed}
type 'a buffered =
    { c : 'a Queue.t Event.channel;
      mutable q : 'a Queue.t;
      size : int }

let pipe () = let c = Event.new_channel () in c, c;;

let size = 1024;;
let out_channel_of_descr chan =
  { c = chan; q = Queue.create (); size = size };;
let in_channel_of_descr = out_channel_of_descr;;

let input_int chan =
  if Queue.length chan.q = 0 then begin
    let q = Event.sync (Event.receive chan.c) in
    if Queue.length q > 0 then chan.q <- q
    else raise End_of_file
  end;
  Queue.take chan.q;;

let flush_out chan =
  if Queue.length chan.q > 0 then Event.sync (Event.send chan.c chan.q);
  chan.q <- Queue.create ();;

let output_int chan x =
  if Queue.length chan.q = size then flush_out chan;
  Queue.add x chan.q

let close_out chan =
  flush_out chan;
  Event.sync (Event.send chan.c chan.q);;
\end{listingcodefile}
%
\begin{codefile}{thbuffercrible.ed}
.
wq
\end{codefile}
%
% This version allows us to regain efficiency comparable to (but not
% better than) the version with pipes.
このバージョンではパイプを使ったバージョンと比較可能な程度の効率を
取り戻すことができます。

% Compared to the original version with processes and pipes, there are
% two potential advantages. First, threads are more lightweight and less
% costly to launch.  Second, communication on a channel merely passes a
% pointer, without copying.  But these advantages are not noticeable
% here, because the number of threads created and the data exchanged are
% not big enough compared to the cost of system calls and compute time.
プロセスとパイプを使ったオリジナルのバージョンと比べると、
チャンネルと使った方法には利点が二つあります。
一つ目はスレッドの起動はプロセスのものと比べてコストがかからないことです。
二つ目はチャンネルを使った通信がデータ全体のコピーではなくポインタの
受け渡しのみを行うことです。
今回の例ではスレッドの数とやり取りされるデータのサイズがシステムコールと計算のコストに比べて
小さいのでこの利点はわかりにくなっています。

% In conclusion, we can say that communication between threads has a
% cost of up to one system call (if the process must be suspended) and
% the cost can be significantly reduced by buffering communication and
% sending larger structures less often.
まとめると、スレッド間の通信は (プロセスが中断される場合) システムコールと同程度の
コストがかかりますが、このコストは通信をバッファして大きな構造体をやり取りすることで
大きく削減できるということになります。
\end{example}

\begin{exercise}[noanswer]
% An {\http} server can be subjected to a high, bursty load.  To improve
% response time, we can refine the architecture of an {\http} server by
% always keeping a dozen threads ready to handle new requests. This means
% that a thread does not handle only a single request, but a potentially
% infinite series of requests that it reads from a queue.
\http サーバには高く激しい負荷がかかることがあります。
応答にかかる時間を減らすために、
新しい接続を処理するためのスレッドを常に数十個保持するように \http
サーバのアーキテクチャを変更することができます。
この変更は各スレッドが一つのリクエストを処理するのではなく、
キューから読んだリクエストを際限なく処理することを意味します。


% To avoid overloading the machine, we can limit the number of threads
% to a reasonable value beyond which the overhead of managing tasks
% exceeds the latency for servicing requests (time spent waiting for
% data on disk, \etc).  After that, we can keep some connections waiting
% to be handled, and then finally we can refuse connections.  When the
% load diminishes and the number of threads is above the \quotes{ideal}
% value, some of them are allowed to die and the others remain ready for
% the next requests.
マシンへの負荷を避けるために、スレッドの数を
タスクを管理する時間がリクエストに対応する時間 (ディスクのデータ待ちの時間など)
よりも小さくなる数に制限する必要があります。
さらに接続が増えた場合は接続を処理待ち状態にしておき、
それ以上に増えた場合は接続を拒否します。
負荷が減りスレッドの数が \quotes{理想的な} 数よりも大きくなれば
スレッドのいくつかを削除します。

% Transform the example of section~\ref{ex/th-relais} into this architecture.
\ref{ex/th-relais} 節の例をこのアーキテクチャに変更してください。
\end{exercise}


% \section{Implementation details}
\section{実装の詳細}

% \paragraph {Implementation of threads in Unix}
\paragraph {Unix におけるスレッドの実装}

% The Unix system was not originally designed to provide support for
% threads.  However, most modern Unix implementations now offer such
% support.  Nevertheless, threads remain an add-on that is sometimes
% apparent.  For example, when using threads it is strongly discouraged
% to use \indexvalue{fork} except when doing \ml+exec+ immediately afterward.
% In effect, \ml+fork+ copies the current thread, which becomes a
% crippled process that runs believing it has threads when in fact they
% do not exist.  The parent continues to run normally as before.
% The special case of a call to \ml+fork+ where the child immediately
% launches another program does not cause the parent any problem.
% Luckily, since that is the only way to start other programs!
もともと Unix システムはスレッドをサポートするようにデザインされていませんでしたが、
ほとんどの現代的な Unix 実装はスレッドをサポートします。
ただしスレッドが後から追加されたものであるということが明白になることがあります。
例えば \ml+exec+ を直後に実行するのでない限り、スレッドを使っているときに
\indexvalue{fork} を使うことは強く非推奨とされています。
\ml+fork+ は現在のスレッドをコピーし、
そのスレッドは他のスレッドが存在すると信じて実行を続けますが、
実際には他のスレッドは存在せず、正しく動作することができません。

\ml+fork+ を実行した親は通常通り実行を続けます。また \ml+fork+ の直後に子プロセスが
他のプログラムを起動する特殊ケースについては親プロセスに問題は起こりません。
この方法で問題が起きないことはラッキーです。なぜならこれが
他のプログラムを起動する唯一の方法だからです。

% Inversely, one can do \ml+fork+ (not followed by \ml+exec+), and then launch
% several threads in the child and the parent, without any problem.
逆に、(\ml+exec+ が続かない) \ml+fork+ の後に
親または子プロセスがスレッドを作成することは問題ありません。

% \paragraph {\label{sec/thread-implementation}Native and simulated implementation in {\ocaml}}
\paragraph {\label{sec/thread-implementation} {\ocaml} におけるネイティブおよびシミュレート実装}

% When the underlying operating system has threads, {\ocaml} can provide
% a native implementation of threads, leaving their management to the
% operating system as much as possible.  Each thread then lives in a
% different Unix process but shares the same address space.
実行されているオペレーティングシステムがスレッドの機能を持つ場合には、
\ocaml はスレッドのネイティブ実装を使います。
このとき \ocaml はオペレーティングシステムにスレッドの管理をできるだけ任せます。
スレッドは同じアドレス空間を共有する異なる Unix プロセスとなります。

% When the system does not provide support for threads, {\ocaml} can
% emulate them.  All the threads then execute in the same Unix process,
% and their management, including their scheduling, is handled by the
% {\ocaml} runtime system.  However, this implementation is only
% available when compiling to bytecode.
システムがスレッドをサポートしない場合には、\ocaml はスレッドをエミュレート
することができます。
全てのスレッドは同じ Unix プロセスで実行されそのスケジューリングを含んだ管理は
\ocaml のランタイムシステムによって行われます。
しかし、この実装はバイトコードにコンパイルするときにのみ利用可能です。

% The {\ocaml} system provides the same programming interface for the
% native and simulated versions of threads. The implementation of
% threads is therefore split: one implementation for the emulated
% version that includes its own task controller, and another
% implementation that is based on \textsc{posix}~(1003.1c) threads and
% lifts the corresponding library functions to the level of the {\ocaml}
% language.  In the process, the {\ocaml} language handles certain
% simple administrative tasks and ensures an interface identical to the
% emulated version.  This guarantees that a program compilable on one
% Unix architecture remains compilable on another Unix architecture.
% However, whether threads are emulated or native can change the
% synchronization of calls to the C library, and therefore change,
% despite everything, the semantics of the program.  It is therefore
% necessary to take certain precautions before believing that a program
% will behave the same way in these two versions.  In this chapter, the
% discussion mainly concern these two implementations, but recall
% that by default, we have taken the viewpoint of a native
% implementation.
\ocaml システムはネイティブとシミュレートされたバージョンのスレッドに同じ
インターフェースを提供しています。
そのためスレッドの実装は二つに分かれています。
一つはエミュレートされたバージョンのためのもので、タスクコントローラを含みます。
もう一つは \textsc{posix}~(1003.1c) に基づくスレッドの実装で、
他の言語で書かれたライブラリ関数を \ocaml へとつなぐものです。
\ocaml は単純な管理タスクを行いインターフェースが
エミュレートされたバージョンと同じことを確認します。
これによってある Unix アーキテクチャでコンパイルできたプログラムがほかの
Unix アーキテクチャでもコンパイルできることが保証されます。

ただしネイティブのスレッドとエミュレートされたスレッドでは同期のための
C ライブラリへの呼び出しが異なるために、プログラムの意味全体が異なってしまいます。
そのためプログラムが二つのバージョンで同じように動くと信じる前にいくつか
対処をしなくてはいけません。この節では
主に二つの実装の違いについて議論しますが、通常はネイティブ実装について語ることを覚えておいてください。

% To use emulated threads, one must pass the \ml+-vmthread+ option
% instead of \ml+-thread+ to the \ml+ocamlc+ compiler. This option is
% not accepted by the \ml+ocamlopt+ compiler.
エミュレートされたスレッドを使うには \ml+ocamlc+ コンパイラに \ml+-thread+ ではなく
\ml+-vmthread+ オプションを渡します。
このオプションを \ml+ocamlopt+ コンパイラに渡すことはできません。

% \paragraph{Sequentialization of {\ocaml} code}
\paragraph{{\ocaml} コードの逐次化}

% The implementation of threads in {\ocaml} must face one of the
% peculiarities of the {\ocaml} language: the automatic management of
% memory and its high consumption of allocated data.  The solution
% adopted, which is the simplest and also generally the most efficient,
% is to sequentialize the execution of {\ocaml} code in all threads: a
% lock in the runtime system prevents two threads from executing
% {\ocaml} code simultaneously.  This seems contrary to the whole idea
% of threads, but it is not, since the lock is released before blocking
% system calls and reacquired upon return.  Other threads can therefore
% take control at that moment.  A special case of such a system call is
% the call to
% \href{http://www.opengroup.org/onlinepubs/007908799/xsh/sched_yield.html}%
% {\texttt{sched\_yield}},
% performed at regular intervals to suspend the running thread and give
% control to another.
\ocaml によるスレッドの実装は自動メモリ管理やアロケートされたデータの頻繁な利用といった
\ocaml の特色を持っている必要があります。
採用されている解法は一番シンプルで最も効率的なもので、\ocaml の全てのスレッドの実行を逐次化するというものです。
つまりランタイムシステムが持つロックによって二つ以上の \ocaml コードを同時に実行できなくします。
これはスレッドという考え方と矛盾するように聞こえますが、そうではありません。
ロックはブロックするシステムコールの直前に開放され、システムコールから返るともう一度取得されるからです。
この間に他のスレッドが実行されます。
このようなシステムコールの特殊なケースは実行中のスレッドで定期的に実行され、他のスレッドに
実行を受け渡す \href{http://www.opengroup.org/onlinepubs/007908799/xsh/sched_yield.html}%
{\texttt{sched\_yield}} の呼び出しです。

% On a multiprocessor machine, the only source of true parallelism comes
% from the execution of C code and system calls.  On a uniprocessor
% machine, the fact that the {\ocaml} code is sequentialized is not
% really noticeable.
マルチプロセッサのマシンでは、本当の並列性は C のコードとシステムコールを
実行することでしか得ることができません。プロセッサが一つしか無いマシンでは、
\ocaml コードが逐次化されたということに気づくことはできません。

% The programmer cannot rely on this sequentialization, because one
% thread can give control to another at almost any moment.  With one
% exception, the sequentialization guarantees memory coherence: two
% threads always have the same view of memory, except perhaps when they
% execute C code.  In effect, the passing of the lock implies a
% synchronization of the memory: a read operation by one thread
% occurring after a write operation to the same address by another
% thread will always return the freshly-written value, with no need for
% additional synchronization.
スレッドの実行はほぼ任意のタイミングで他のスレッドに移るので、
プログラマはこの逐次化を頼ることはできません。
ただし例外として、逐次化されたコードではメモリの一貫性が保証されます。
つまり、C のコードを実行するときなどを除いて二つのスレッドは同じメモリを目にするということです。
その結果として、ロックの受け渡しがメモリの同期を意味するようになります。
あるスレッドが書き込んだアドレスを別のスレッドが読み込んだ場合
常に新しい値が読み込まれ、同期処理は必要ありません。

% \paragraph {Threads and signals}
\paragraph {スレッドとシグナル}

% Generally speaking, using signals is already delicate with a single
% thread due to their asynchronous character.  It is even more so in the
% presence of multiple threads because of the addition of new
% difficulties: which thread should a signal be sent to?  To all, to the
% primary one, or to the one currently running?  What happens if one
% thread sends a signal to another?  In fact, threads were implemented
% before answering these questions, and different implementations can
% behave differently with respect to signals.
一般的に言って、非同期という特性を持つシグナルを使うことはシングルスレッドの
プログラムでさえ注意を要する難しいものでした。
シグナルはマルチスレッドのプログラムではさらに難しくなります。
どのスレッドにシグナルが送られるべきでしょうか?
全てのスレッド、主スレッド、それとも現在実行しているスレッド?
あるスレッドが別のスレッドにシグナルを送りたい場合どうすればいいでしょうか?
実際のところ、スレッドはこのような問題に答えることなく実装されました。
そのため実装によってシグナルへの動作は異なります。

% The \ml+Thread.join+, \ml+Mutex.lock+, and \ml+Condition.wait+ functions
% despite being long system calls, are not interruptible by a signal.
% (They cannot therefore fail with the \ml+EINTR+ error.)  If a signal
% is sent while waiting, it will be received and handled when the call
% returns.
\ml+Thread.join+ と \ml+Mutex.lock+ そして \ml+Condition.wait+ は
遅いシステムコールにもかかわらずシグナルによって中断されることがありません
(したがって \ml+EINTR+ エラーを出して失敗することもありません)。
待っているときにシグナルが送られて場合、そのシグナルはシステムコールから返ってから
受け取られて処理されます。

% The \textsc{posix} standard specifies that the signal handler is
% shared among all the threads and in contrast the signal mask is
% private to each thread and inherited upon creation of a thread.  But
% the behavior of threads with respect to signals remains largely
% underspecified and therefore non-portable.
\textsc{posix} 規格はシグナルハンドラがスレッド間で共有されることと
シグナルマスクはスレッドの作成時にもとのスレッドから継承された後はスレッドに
プライベートとなることを定めています。
しかしシグナルに対するスレッドの動作は主に定められておらず、ポータブルにはなっていません。

% It is therefore preferable to avoid as much as possible the use of
% asynchronous signals (such as \ml+sigalrm+, \ml+sigvtalrm+,
% \ml+sigchld+, \etc) with threads. These can be blocked and examined
% with \ml+Thread.+\libvalue{Thread}{wait\_signal}.
% We can dedicate a thread to signal
% handling and nothing else: it can wait for the reception of signals,
% undertake the necessary actions, and update certain information
% examined by other threads.
したがってスレッドではできる限り非同期シグナル
(\ml+sigalrm+ や \ml+sigvtalrm+、 \ml+sigchld+ など) を避けることが望ましいです。
シグナルは \ml+Thread.+\libvalue{Thread}{wait\_signal} によってブロックと
調査をすることができます。
シグナルの処理だけを行うスレッドを作ることもできます。
このスレッドはシグナルを待ち、適切な処理を行い、他のスレッドにシグナルの情報を伝えます。

% In addition, {\ocaml} threads (since version 3.08) use the
% \ml+sigvtalarm+ signal internally to implement preemption of threads.
% This signal is therefore reserved and must not be used by the program
% itself, since there is a risk of interference.
加えて、(バージョン 3.08 以降の) \ocaml のスレッドはスレッドのプリエンプションに
内部で \ml+sigvtalarm+ を使います。
そのためこのシグナルを使うとシステムと干渉する可能性があるために、
\ml+sigvtalarm+ シグナルはプログラムの中で使うことができません。

\begin{codefile}{thmisc.ed}
f th/misc.mli
r misc.mli
$r th/misc.MLI
w
f th/misc.ml
1,$ d
r misc.ml
$r th/misc.ML
wq
\end{codefile}
